{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac662ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ctypes\n",
    "\n",
    "# Some constants taken from cuda.h\n",
    "CUDA_SUCCESS = 0\n",
    "CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT = 16\n",
    "CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR = 39\n",
    "CU_DEVICE_ATTRIBUTE_CLOCK_RATE = 13\n",
    "CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a5fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertSMVer2Cores(major, minor):\n",
    "    # Returns the number of CUDA cores per multiprocessor for a given\n",
    "    # Compute Capability version. There is no way to retrieve that via\n",
    "    # the API, so it needs to be hard-coded.\n",
    "    # See _ConvertSMVer2Cores in helper_cuda.h in NVIDIA's CUDA Samples.\n",
    "    return {(1, 0): 8,    # Tesla\n",
    "            (1, 1): 8,\n",
    "            (1, 2): 8,\n",
    "            (1, 3): 8,\n",
    "            (2, 0): 32,   # Fermi\n",
    "            (2, 1): 48,\n",
    "            (3, 0): 192,  # Kepler\n",
    "            (3, 2): 192,\n",
    "            (3, 5): 192,\n",
    "            (3, 7): 192,\n",
    "            (5, 0): 128,  # Maxwell\n",
    "            (5, 2): 128,\n",
    "            (5, 3): 128,\n",
    "            (6, 0): 64,   # Pascal\n",
    "            (6, 1): 128,\n",
    "            (6, 2): 128,\n",
    "            (7, 0): 64,   # Volta\n",
    "            (7, 2): 64,\n",
    "            (7, 5): 64,   # Turing\n",
    "            (8, 0): 64,   # Ampere\n",
    "            (8, 6): 64,\n",
    "            }.get((major, minor), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02efdce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cuda_available():\n",
    "    libnames = ('libcuda.so', 'libcuda.dylib', 'cuda.dll')\n",
    "    for libname in libnames:\n",
    "        try:\n",
    "            cuda = ctypes.CDLL(libname)\n",
    "        except OSError:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    result = ctypes.c_int()\n",
    "    result = cuda.cuInit(0)\n",
    "    if result == CUDA_SUCCESS:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2524b264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cuda_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2732d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_cuda_devices():\n",
    "    libnames = ('libcuda.so', 'libcuda.dylib', 'cuda.dll')\n",
    "    for libname in libnames:\n",
    "        try:\n",
    "            cuda = ctypes.CDLL(libname)\n",
    "        except OSError:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        raise OSError(\"could not load any of: \" + ' '.join(libnames))\n",
    "    \n",
    "    nGpus = ctypes.c_int()\n",
    "    name = b' ' * 100\n",
    "    cc_major = ctypes.c_int()\n",
    "    cc_minor = ctypes.c_int()\n",
    "    cores = ctypes.c_int()\n",
    "    threads_per_core = ctypes.c_int()\n",
    "    clockrate = ctypes.c_int()\n",
    "    freeMem = ctypes.c_size_t()\n",
    "    totalMem = ctypes.c_size_t()\n",
    "\n",
    "    device = ctypes.c_int()\n",
    "    context = ctypes.c_void_p()\n",
    "    error_str = ctypes.c_char_p()\n",
    "    result = ctypes.c_int()\n",
    "    result = cuda.cuInit(0)\n",
    "\n",
    "    if result != CUDA_SUCCESS:\n",
    "        cuda.cuGetErrorString(result, ctypes.byref(error_str))\n",
    "        print(\"cuInit failed with error code %d: %s\" % (result, error_str.value.decode()))\n",
    "        return 1\n",
    "    result = cuda.cuDeviceGetCount(ctypes.byref(nGpus))\n",
    "    if result != CUDA_SUCCESS:\n",
    "        cuda.cuGetErrorString(result, ctypes.byref(error_str))\n",
    "        print(\"cuDeviceGetCount failed with error code %d: %s\" % (result, error_str.value.decode()))\n",
    "        return 1\n",
    "    print(\"Found %d device(s).\" % nGpus.value)\n",
    "    for i in range(nGpus.value):\n",
    "        result = cuda.cuDeviceGet(ctypes.byref(device), i)\n",
    "        if result != CUDA_SUCCESS:\n",
    "            cuda.cuGetErrorString(result, ctypes.byref(error_str))\n",
    "            print(\"cuDeviceGet failed with error code %d: %s\" % (result, error_str.value.decode()))\n",
    "            return 1\n",
    "        print(\"Device: %d\" % i)\n",
    "        if cuda.cuDeviceGetName(ctypes.c_char_p(name), len(name), device) == CUDA_SUCCESS:\n",
    "            print(\"  Name: %s\" % (name.split(b'\\0', 1)[0].decode()))\n",
    "        if cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device) == CUDA_SUCCESS:\n",
    "            print(\"  Compute Capability: %d.%d\" % (cc_major.value, cc_minor.value))\n",
    "        if cuda.cuDeviceGetAttribute(ctypes.byref(cores), CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, device) == CUDA_SUCCESS:\n",
    "            print(\"  Multiprocessors: %d\" % cores.value)\n",
    "            print(\"  CUDA Cores: %s\" % (cores.value * ConvertSMVer2Cores(cc_major.value, cc_minor.value) or \"unknown\"))\n",
    "            if cuda.cuDeviceGetAttribute(ctypes.byref(threads_per_core), CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR, device) == CUDA_SUCCESS:\n",
    "                print(\"  Concurrent threads: %d\" % (cores.value * threads_per_core.value))\n",
    "        if cuda.cuDeviceGetAttribute(ctypes.byref(clockrate), CU_DEVICE_ATTRIBUTE_CLOCK_RATE, device) == CUDA_SUCCESS:\n",
    "            print(\"  GPU clock: %g MHz\" % (clockrate.value / 1000.))\n",
    "        if cuda.cuDeviceGetAttribute(ctypes.byref(clockrate), CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE, device) == CUDA_SUCCESS:\n",
    "            print(\"  Memory clock: %g MHz\" % (clockrate.value / 1000.))\n",
    "        try:\n",
    "            result = cuda.cuCtxCreate_v2(ctypes.byref(context), 0, device)\n",
    "        except AttributeError:\n",
    "            result = cuda.cuCtxCreate(ctypes.byref(context), 0, device)\n",
    "        if result != CUDA_SUCCESS:\n",
    "            cuda.cuGetErrorString(result, ctypes.byref(error_str))\n",
    "            print(\"cuCtxCreate failed with error code %d: %s\" % (result, error_str.value.decode()))\n",
    "        else:\n",
    "            try:\n",
    "                result = cuda.cuMemGetInfo_v2(ctypes.byref(freeMem), ctypes.byref(totalMem))\n",
    "            except AttributeError:\n",
    "                result = cuda.cuMemGetInfo(ctypes.byref(freeMem), ctypes.byref(totalMem))\n",
    "            if result == CUDA_SUCCESS:\n",
    "                print(\"  Total Memory: %ld MiB\" % (totalMem.value / 1024**2))\n",
    "                print(\"  Free Memory: %ld MiB\" % (freeMem.value / 1024**2))\n",
    "            else:\n",
    "                cuda.cuGetErrorString(result, ctypes.byref(error_str))\n",
    "                print(\"cuMemGetInfo failed with error code %d: %s\" % (result, error_str.value.decode()))\n",
    "            cuda.cuCtxDetach(context)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ec3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 device(s).\n",
      "Device: 0\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n",
      "Device: 1\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n",
      "Device: 2\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n",
      "Device: 3\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n",
      "Device: 4\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n",
      "Device: 5\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n",
      "Device: 6\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n",
      "Device: 7\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n",
      "Device: 8\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n",
      "Device: 9\n",
      "  Name: NVIDIA GeForce RTX 2080 Ti\n",
      "  Compute Capability: 7.5\n",
      "  Multiprocessors: 68\n",
      "  CUDA Cores: 4352\n",
      "  Concurrent threads: 69632\n",
      "  GPU clock: 1545 MHz\n",
      "  Memory clock: 7000 MHz\n",
      "  Total Memory: 11011 MiB\n",
      "  Free Memory: 10853 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_cuda_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a748ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63eec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
